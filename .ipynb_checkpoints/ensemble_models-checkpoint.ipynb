{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble models for classifying wearing glasses or not from GAN generated images "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This projects is the extension of the previous Bayesian optimization. The same dataset and objectives have been adopted. But I will use a different model strategies to determine if a person is wearing glasses or not using data contianing all features. These features are generated from a Generative Adversarial Neural Network (GAN). The detailed information can refer to [here](https://www.kaggle.com/jeffheaton/glasses-or-no-glasses).\n",
    "\n",
    "In the second step, we have determined the right NN model for the analysis using Bayesian optimization. Based on that results, we will build an ensembler with multiple models such as RF, KNN and gradient boosting methods to conduct prediction with 10-fold cross validation.\n",
    "\n",
    "### Overview of following steps\n",
    "\n",
    "**1** Base on optimized parameters such as dropout,neuronPct, neuronShrink to form the NN models.\n",
    "\n",
    "**2** Build a model ensembler with seven models and 10 folds cross validation.\n",
    "\n",
    "**3** For each fold, used 9/10 of training data (9 folds) to build the model, the rest to conduct prediction. Meanwhile, we also used the trained model to predict at the test data. Here, different strategies have been applied to collect the validation prediction results (from cv of training data) and submit prediction results. \n",
    "\n",
    "Regarding the **validation prediction results**, it need to collect in each fold as we only have 1/10 of training data to obtian the prediction results. As follows, the test will tell you which row index we should fill in. At the end of 10 folds, we should have every values in the one column (represent one model).\n",
    "```    \n",
    "                                     dataset_blend_train[test, j] = pred[:, 1]\n",
    "```\n",
    "Regarding the **test prediction results**, we collected the prediction results for the submit data for each fold and each model. At the end of each model, we need to do avergae of these prediction results of these 10 folds.\n",
    "\n",
    "**4** Build a logstic regresson between y and the validation prediction results. Use this model to conduct the prediction of our submit prediction results. This results is more like to assign weights to these models for prediction of final output.\n",
    "\n",
    "**5** Format te output and save results.\n",
    "\n",
    "### Data\n",
    "There are two data were used for this project:\n",
    "\n",
    "**(1)** training.csv, which include the 512 features one response variable glasses (1 represent have glass, 0 means no glasses). \n",
    "\n",
    "**(2)** submit.csv, which in fact is the test data which to measure how good of the model.\n",
    "\n",
    "\n",
    "**Objetive**: To develop a robust approach to conduct classification on data (a person is wearing glasses or not) using a ensemble of models, which include machine learning models (random forest,Gradient Boosting and Extra Trees) and deep learning model (optimized NN using Bayesian optimization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow.keras.initializers\n",
    "import statistics\n",
    "import tensorflow.keras\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, InputLayer,BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from tensorflow.keras.layers import LeakyReLU,PReLU\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use optmized NN model's parameters from previous Bayesian optimization\n",
    "\n",
    "Based on previous Bayesian Optimization, we can chose the optimzied parameters to build a NN model\n",
    "\n",
    "{'dropout': 0.07323118951773941,\n",
    " 'lr': 0.009233859476879781,\n",
    " 'neuronPct': 0.1943976092638942,\n",
    " 'neuronShrink': 0.35210511977261727}\n",
    " \n",
    "### Optimized NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##revised from generate_model functon\n",
    "dropout = 0.07323118951773941\n",
    "neuronPct = 0.1943976092638942\n",
    "neuronShrink = 0.35210511977261727\n",
    "lr = 0.01863\n",
    "\n",
    "def nn_model(dropout, neuronPct, neuronShrink,num_class,lr,init_num_neurons = 4000):\n",
    "    # We start with some percent of 5000 starting neurons on the first hidden layer.\n",
    "    neuronCount = int(neuronPct * init_num_neurons)\n",
    "    \n",
    "    # Construct neural network\n",
    "    # kernel_initializer = tensorflow.keras.initializers.he_uniform(seed=None)\n",
    "    model = Sequential()\n",
    "\n",
    "    # So long as there would have been at least 25 neurons and fewer than 10\n",
    "    # layers, create a new layer.\n",
    "    layer = 0\n",
    "    while neuronCount>25 and layer<10:\n",
    "        # The first (0th) layer needs an input input_dim(neuronCount)\n",
    "        if layer==0:\n",
    "            model.add(Dense(neuronCount, \n",
    "                input_dim=x.shape[1], \n",
    "                activation=PReLU()))\n",
    "        else:\n",
    "            model.add(Dense(neuronCount, activation=PReLU())) \n",
    "            \n",
    "        layer += 1\n",
    "        model.add(BatchNormalization())\n",
    "        # Add dropout after each hidden layer\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        # Shrink neuron count for each layer\n",
    "        neuronCount = neuronCount * neuronShrink\n",
    "\n",
    "    if num_class>2:\n",
    "        \n",
    "        model.add(Dense(num_class,activation='softmax')) # Output\n",
    "    ##new added part\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=lr))\n",
    "    else:\n",
    "        model.add(Dense(num_class,activation='sigmoid')) # Output\n",
    "        \n",
    "        ##new added part\n",
    "        model.compile(loss='binary_crossentropy', optimizer=Adam(lr=lr))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Model: 0 : <tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x0000029C3302EA08>\n",
      "Train on 4050 samples\n",
      "4050/4050 [==============================] - 5s 1ms/sample - loss: 0.0988\n",
      "Fold #0: loss=0.19948168075353725\n",
      "Train on 4050 samples\n",
      "4050/4050 [==============================] - 4s 929us/sample - loss: 0.1053\n",
      "Fold #1: loss=0.2835264015548779\n",
      "Train on 4050 samples\n",
      "4050/4050 [==============================] - 4s 880us/sample - loss: 0.1190\n",
      "Fold #2: loss=0.28316105559833993\n",
      "Train on 4050 samples\n",
      "4050/4050 [==============================] - 4s 883us/sample - loss: 0.1242\n",
      "Fold #3: loss=0.2954316608935089\n",
      "Train on 4050 samples\n",
      "4050/4050 [==============================] - 4s 900us/sample - loss: 0.1077\n",
      "Fold #4: loss=0.33633542314876386\n",
      "Train on 4050 samples\n",
      "4050/4050 [==============================] - 4s 883us/sample - loss: 0.1216\n",
      "Fold #5: loss=0.19641085826017335\n",
      "Train on 4050 samples\n",
      "4050/4050 [==============================] - 4s 914us/sample - loss: 0.1195\n",
      "Fold #6: loss=0.2701019382864923\n",
      "Train on 4050 samples\n",
      "4050/4050 [==============================] - 4s 929us/sample - loss: 0.1186\n",
      "Fold #7: loss=0.2734718042067352\n",
      "Train on 4050 samples\n",
      "4050/4050 [==============================] - 4s 888us/sample - loss: 0.1000\n",
      "Fold #8: loss=0.29325654197510176\n",
      "Train on 4050 samples\n",
      "4050/4050 [==============================] - 4s 917us/sample - loss: 0.1109\n",
      "Fold #9: loss=0.38209583728065244\n",
      "KerasClassifier: Mean loss=0.2813273201958183\n",
      "Model: 1 : KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
      "                     weights='uniform')\n",
      "Fold #0: loss=0.0018020671471483923\n",
      "Fold #1: loss=0.005144461362206777\n",
      "Fold #2: loss=0.006045494935780472\n",
      "Fold #3: loss=0.0027031007207220887\n",
      "Fold #4: loss=9.992007221626413e-16\n",
      "Fold #5: loss=0.07765387000670954\n",
      "Fold #6: loss=0.006946528509354171\n",
      "Fold #7: loss=0.0018020671471483923\n",
      "Fold #8: loss=0.0060454949357804716\n",
      "Fold #9: loss=0.004505167867869483\n",
      "KNeighborsClassifier: Mean loss=0.01126482526327208\n",
      "Model: 2 : RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
      "                       n_jobs=-1, oob_score=False, random_state=None, verbose=0,\n",
      "                       warm_start=False)\n",
      "Fold #0: loss=0.3367388265516488\n",
      "Fold #1: loss=0.33660141867386795\n",
      "Fold #2: loss=0.33767086761957854\n",
      "Fold #3: loss=0.3468523239402217\n",
      "Fold #4: loss=0.33818952547277825\n",
      "Fold #5: loss=0.33451364343904905\n",
      "Fold #6: loss=0.33882269959087447\n",
      "Fold #7: loss=0.33100046318406445\n",
      "Fold #8: loss=0.3353765178632944\n",
      "Fold #9: loss=0.33206660733472293\n",
      "RandomForestClassifier: Mean loss=0.3367832893670101\n",
      "Model: 3 : RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='entropy', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
      "                       n_jobs=-1, oob_score=False, random_state=None, verbose=0,\n",
      "                       warm_start=False)\n",
      "Fold #0: loss=0.34391005266475116\n",
      "Fold #1: loss=0.33821491146501254\n",
      "Fold #2: loss=0.34271869698071655\n",
      "Fold #3: loss=0.3472067587885085\n",
      "Fold #4: loss=0.3385638635849771\n",
      "Fold #5: loss=0.33978750965572735\n",
      "Fold #6: loss=0.34310651306104745\n",
      "Fold #7: loss=0.3334637516409057\n",
      "Fold #8: loss=0.3424623200375926\n",
      "Fold #9: loss=0.3374244392399384\n",
      "RandomForestClassifier: Mean loss=0.3406858817119177\n",
      "Model: 4 : ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
      "                     criterion='gini', max_depth=None, max_features='auto',\n",
      "                     max_leaf_nodes=None, max_samples=None,\n",
      "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                     min_samples_leaf=1, min_samples_split=2,\n",
      "                     min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=-1,\n",
      "                     oob_score=False, random_state=None, verbose=0,\n",
      "                     warm_start=False)\n",
      "Fold #0: loss=0.3487580007912734\n",
      "Fold #1: loss=0.3498233657781153\n",
      "Fold #2: loss=0.3497499819184618\n",
      "Fold #3: loss=0.3544834206827738\n",
      "Fold #4: loss=0.3466741726434989\n",
      "Fold #5: loss=0.3458601782985981\n",
      "Fold #6: loss=0.34757565261646084\n",
      "Fold #7: loss=0.34184885410481153\n",
      "Fold #8: loss=0.3506247956853842\n",
      "Fold #9: loss=0.3455706728517203\n",
      "ExtraTreesClassifier: Mean loss=0.34809690953710976\n",
      "Model: 5 : ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
      "                     criterion='entropy', max_depth=None, max_features='auto',\n",
      "                     max_leaf_nodes=None, max_samples=None,\n",
      "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                     min_samples_leaf=1, min_samples_split=2,\n",
      "                     min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=-1,\n",
      "                     oob_score=False, random_state=None, verbose=0,\n",
      "                     warm_start=False)\n",
      "Fold #0: loss=0.34300305917182755\n",
      "Fold #1: loss=0.34379039773811\n",
      "Fold #2: loss=0.34546193486673005\n",
      "Fold #3: loss=0.3559062682036947\n",
      "Fold #4: loss=0.34280163261050023\n",
      "Fold #5: loss=0.3452610614076208\n",
      "Fold #6: loss=0.3422745490508535\n",
      "Fold #7: loss=0.34082284685706593\n",
      "Fold #8: loss=0.3425977417226247\n",
      "Fold #9: loss=0.3471503274531902\n",
      "ExtraTreesClassifier: Mean loss=0.3449069819082218\n",
      "Model: 6 : GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.05, loss='deviance', max_depth=6,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=50,\n",
      "                           n_iter_no_change=None, presort='deprecated',\n",
      "                           random_state=None, subsample=0.5, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n",
      "Fold #0: loss=0.3107423980272093\n",
      "Fold #1: loss=0.2987206459097164\n",
      "Fold #2: loss=0.29412761035741375\n",
      "Fold #3: loss=0.29296052788279414\n",
      "Fold #4: loss=0.28667475323319663\n",
      "Fold #5: loss=0.2823984570636409\n",
      "Fold #6: loss=0.29215478055686467\n",
      "Fold #7: loss=0.28992094906518484\n",
      "Fold #8: loss=0.3016788208005262\n",
      "Fold #9: loss=0.2931964434538423\n",
      "GradientBoostingClassifier: Mean loss=0.29425753863503895\n",
      "\n",
      "Blending models.\n"
     ]
    }
   ],
   "source": [
    "PATH = './data' ##input\n",
    "outpath = './data/submit' ##output\n",
    "\n",
    "SHUFFLE = False\n",
    "FOLDS = 10\n",
    "\n",
    "## in our case, we didn't really use this function\n",
    "# def mlogloss(y_test, preds):\n",
    "#     epsilon = 1e-15\n",
    "#     sum = 0\n",
    "#     for row in zip(preds,y_test):\n",
    "#         x = row[0][row[1]]\n",
    "#         x = max(epsilon,x)\n",
    "#         x = min(1-epsilon,x)\n",
    "#         sum+=math.log(x)\n",
    "#     return( (-1/len(preds))*sum)\n",
    "\n",
    "def stretch(y):\n",
    "    return (y - y.min()) / (y.max() - y.min())\n",
    "\n",
    "\n",
    "def blend_ensemble(x, y, x_submit):\n",
    "    kf = StratifiedKFold(FOLDS)\n",
    "    #folds = list(kf.split(x,y[:,0]))  # '''this is very important as y should be 1-D arrary'''\n",
    "    folds = list(kf.split(x,y))\n",
    "    \n",
    "    models = [\n",
    "        KerasClassifier(build_fn=nn_model,dropout = dropout, neuronPct = neuronPct, \n",
    "                        neuronShrink = neuronShrink,num_class = num_class,lr=lr), ##definided NN model with customized parameters\n",
    "        KNeighborsClassifier(n_neighbors=3),\n",
    "        RandomForestClassifier(n_estimators=200, n_jobs=-1, criterion='gini'),\n",
    "        RandomForestClassifier(n_estimators=200, n_jobs=-1, criterion='entropy'),\n",
    "        ExtraTreesClassifier(n_estimators=200, n_jobs=-1, criterion='gini'),\n",
    "        ExtraTreesClassifier(n_estimators=200, n_jobs=-1, criterion='entropy'),\n",
    "        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=50)]\n",
    "\n",
    "    dataset_blend_train = np.zeros((x.shape[0], len(models)))\n",
    "    dataset_blend_test = np.zeros((x_submit.shape[0], len(models)))\n",
    "\n",
    "    for j, model in enumerate(models):\n",
    "        print(\"Model: {} : {}\".format(j, model) )\n",
    "        fold_sums = np.zeros((x_submit.shape[0], len(folds)))\n",
    "        total_loss = 0\n",
    "        for i, (train, test) in enumerate(folds):\n",
    "            x_train = x[train]\n",
    "            y_train = y[train]\n",
    "            x_test = x[test]\n",
    "            y_test = y[test]\n",
    "            \n",
    "            if j==0: ## as it nrequire different formats for our definied model\n",
    "                tem = pd.DataFrame(y[train],columns=['yp'])\n",
    "                y_train = pd.get_dummies(tem['yp']).values\n",
    "                \n",
    "                tem1 = pd.DataFrame(y[test],columns=['yp'])\n",
    "                y_test = pd.get_dummies(tem1['yp']).values\n",
    "                \n",
    "            model.fit(x_train, y_train)\n",
    "            pred = np.array(model.predict_proba(x_test)) ## it didn't work well for KNN as it will generate three dimension results\n",
    "            \n",
    "            #if len(pred.shape) == 3:\n",
    "            #    pred = pred[1]\n",
    "            #pred = np.array(model.predict(x_test))\n",
    "            # pred = model.predict_proba(x_test)\n",
    "            dataset_blend_train[test, j] = pred[:, 1]\n",
    "            pred2 = np.array(model.predict_proba(x_submit))\n",
    "            #if len(pred2.shape) == 3:\n",
    "            #    pred2 = pred2[1]\n",
    "            #fold_sums[:, i] = model.predict_proba(x_submit)[:, 1]\n",
    "            fold_sums[:, i] = pred2[:, 1]\n",
    "            loss = metrics.log_loss(y_test, pred) ##here we ued logloss from the function, not our own defined\n",
    "            total_loss+=loss\n",
    "            print(\"Fold #{}: loss={}\".format(i,loss))\n",
    "        print(\"{}: Mean loss={}\".format(model.__class__.__name__,total_loss/len(folds)))\n",
    "        dataset_blend_test[:, j] = fold_sums.mean(1)\n",
    "\n",
    "    print()\n",
    "    print(\"Blending models.\")\n",
    "    blend = LogisticRegression(solver='lbfgs')\n",
    "    blend.fit(dataset_blend_train, y)\n",
    "    return blend.predict_proba(dataset_blend_test)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    np.random.seed(2)  # seed to shuffle the train set\n",
    "\n",
    "    print(\"Loading data...\")\n",
    "    filename_train = os.path.join(PATH, \"train.csv\")\n",
    "    df_train = pd.read_csv(filename_train, na_values=['NA', '?'])\n",
    "    df_train.drop('id',axis =1, inplace = True)\n",
    "    num_class = df_train['glasses'].nunique()\n",
    "    \n",
    "    filename_submit = os.path.join(PATH, \"test.csv\")\n",
    "    df_submit = pd.read_csv(filename_submit, na_values=['NA', '?'])\n",
    "    ids = df_submit['id']\n",
    "    df_submit.drop('id',axis =1, inplace = True)\n",
    "    \n",
    "    \n",
    "    predictors = list(df_train.columns.values)\n",
    "    predictors.remove('glasses')\n",
    "    x = df_train[predictors].values\n",
    "    y = df_train['glasses'].values\n",
    "    #dummies = pd.get_dummies(df_train['glasses']) # Classification\n",
    "    #y = dummies.values\n",
    "    \n",
    "    \n",
    "    x_submit = df_submit.values\n",
    "\n",
    "    if SHUFFLE:\n",
    "        idx = np.random.permutation(y.size)\n",
    "        x = x[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "    submit_data = blend_ensemble(x, y, x_submit)\n",
    "    submit_data = stretch(submit_data)\n",
    "\n",
    "    ####################\n",
    "    # Build submit file\n",
    "    ####################\n",
    "    #ids = [id+1 for id in range(submit_data.shape[0])]\n",
    "    submit_filename = os.path.join(outpath, \"submit_nn_ensembles.csv\")\n",
    "    submit_df = pd.DataFrame({'id': ids, 'glasses': submit_data[:, 1]},\n",
    "                             columns=['id','glasses'])\n",
    "    submit_df.to_csv(submit_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
